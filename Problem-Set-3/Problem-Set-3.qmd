---
title: "Problem Set 3"
format: pdf
---

# 1.
```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
import os
import statsmodels.api as sm
import seaborn as sns

os.chdir("C:/Users/danie/Documents/GitHub/Machine-Learning--Harris/Problem-set-3")

default = pd.read_csv("Data-Default.csv")
default["student"] = np.where(default["student"] == "No", 0, 1)
default["default"] = np.where(default["default"] == "Yes", 1, 0)
default = default.reset_index()

```


(a)
```{python}
X = default[["income", "balance"]]
X = sm.add_constant(X)

model = sm.Logit(default["default"], X)
results = model.fit()
results.summary()
```

The standard error for income and balance are exceptionally small, with "income's" SE at 4.99e-06, and "balance's" SE
so small the summary function is regestering it as 0. 


(b)
```{python}

def get_indices(data, n, seed):
    rng = np.random.default_rng(seed)    # allows you to set your seed
    indices = rng.choice(data.index,     # Use the dataset's indices as the input
                         int(n),         # Number of indices per sample
                         replace=True    # Draw samples with replacement
                        )
    return indices

def boot_fn(df, index):
    sampled_df = df.loc[index]
    y = sampled_df["default"]
    x = sampled_df[["income", "balance"]]
    x = sm.add_constant(x)

    model = sm.Logit(y, x)
    results = model.fit(disp = False)
    coefficients = results.params
    income = coefficients.iloc[1]
    balance = coefficients.iloc[2]
    return income, balance

income, balance = boot_fn(default, get_indices(default, 1000, 32))

print(f"income: {income}")
print(f"balance: {balance}")


```

(c)
```{python}
bootstrap_means_income = np.zeros(1000)
bootstrap_means_balance = np.zeros(1000)

for i in range(1000):
    chosen_index = np.random.choice(default["index"], size = default.shape[0], replace = True)
    sample = default.loc[chosen_index]
    result = boot_fn(sample, sample["index"])
    bootstrap_means_income[i] = result[0]
    bootstrap_means_balance[i] = result[1]

print()
print(f"Mean coefficient value, income:  {np.mean(bootstrap_means_income)}")
print(f"Mean coefficient value, balance: {np.mean(bootstrap_means_balance)}")
print()
print(f"Std Income: {np.std(bootstrap_means_income)}")
print(f"Std Balance: {np.std(bootstrap_means_balance)}")

```

(d)
These values are nearly spot on, with the standard errors being very similar as well. The only thing ot notice is that the standard error of the inome variable using bootstrapping is signfiicantly larger than that provided via the regression estimate, which indicates that there is more uncertainty from the bootstrapping method. 

# 2. 
(a)
```{python}
rng = np.random.default_rng(1)
x = rng.normal(size = 100)
y = x - 2 * x**2 + rng.normal(size = 100)

```

n is the size of the dataset, which in this case is 100. P is the parameters, of which there are 2, X, and x^2. 

(b)
```{python}
plt.scatter(x, y)
plt.show()
```

The relationship between the two variables is parabolic (which is exactly to be expected) and that there is some grouping of variables between -1 and 1 (which is coming from the normal variation we included in the rng.normal() command.)